{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 3.250255,
     "end_time": "2025-02-20T05:12:56.861553",
     "exception": false,
     "start_time": "2025-02-20T05:12:53.611298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss, brier_score_loss, mean_squared_error, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.base import clone\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 0.010506,
     "end_time": "2025-02-20T05:12:56.880965",
     "exception": false,
     "start_time": "2025-02-20T05:12:56.870459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/march-machine-learning-mania-2025/**'\n",
    "data_dir ='data/**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.180698,
     "end_time": "2025-02-20T05:12:57.065231",
     "exception": false,
     "start_time": "2025-02-20T05:12:56.884533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TournamentPredictor:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_path = data_dir\n",
    "        self.data = None\n",
    "        self.teams = None\n",
    "        self.seeds = None\n",
    "        self.games = None\n",
    "        self.sub = None\n",
    "        self.gb = None\n",
    "        self.col = None\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def load_data(self):\n",
    "        files = glob.glob(self.data_path)\n",
    "        self.data = {p.split('/')[-1].split('.')[0]: pd.read_csv(p, encoding='latin-1') for p in files}\n",
    "\n",
    "        teams = pd.concat([self.data['MTeams'], self.data['WTeams']])\n",
    "        teams_spelling = pd.concat([self.data['MTeamSpellings'], self.data['WTeamSpellings']])\n",
    "        teams_spelling = teams_spelling.groupby(by='TeamID', as_index=False)['TeamNameSpelling'].count()\n",
    "        teams_spelling.columns = ['TeamID', 'TeamNameCount']\n",
    "        self.teams = pd.merge(teams, teams_spelling, how='left', on=['TeamID'])\n",
    "\n",
    "        season_cresults = pd.concat([self.data['MRegularSeasonCompactResults'], self.data['WRegularSeasonCompactResults']])\n",
    "        season_dresults = pd.concat([self.data['MRegularSeasonDetailedResults'], self.data['WRegularSeasonDetailedResults']])\n",
    "        tourney_cresults = pd.concat([self.data['MNCAATourneyCompactResults'], self.data['WNCAATourneyCompactResults']])\n",
    "    \n",
    "        tourney_dresults = pd.concat([self.data['MNCAATourneyDetailedResults'], self.data['WNCAATourneyDetailedResults']])\n",
    "\n",
    "        seeds_df = pd.concat([self.data['MNCAATourneySeeds'], self.data['WNCAATourneySeeds']])\n",
    "        self.seeds = {'_'.join(map(str, [int(k1), k2])): int(v[1:3]) for k1, v, k2 in seeds_df[['Season', 'Seed', 'TeamID']].values}\n",
    "\n",
    "        \n",
    "\n",
    "        season_cresults['ST'] = 'S'\n",
    "        season_dresults['ST'] = 'S'\n",
    "        tourney_cresults['ST'] = 'T'\n",
    "        tourney_dresults['ST'] = 'T'\n",
    "\n",
    "        self.games = pd.concat((season_dresults, tourney_dresults), axis=0, ignore_index=True)\n",
    "        self.games['Team1'] = self.games.apply(lambda r: sorted([r['WTeamID'], r['LTeamID']])[0], axis=1)\n",
    "        self.games['Team2'] = self.games.apply(lambda r: sorted([r['WTeamID'], r['LTeamID']])[1], axis=1)\n",
    "        self.games['Pred'] = self.games.apply(lambda r: 1.0 if sorted([r['WTeamID'], r['LTeamID']])[0] == r['WTeamID'] else 0.0, axis=1)\n",
    "        #restrict to only tournament data (faster & better) // edit: wrong bc we only predict like 4 games\n",
    "        self.teamsResults = self.getTeamStats(self.games)\n",
    "\n",
    "\n",
    "        # merge games with team results\n",
    "        self.games = self.games[['Season', 'DayNum', 'Team1', 'Team2','Pred']]\n",
    "        self.games = pd.merge(self.games, self.teamsResults, how='left', left_on=['Season', 'DayNum', 'Team1'], right_on=['Season','DayNum','TeamID'], suffixes=('', '_1'))\n",
    "        self.games = pd.merge(self.games, self.teamsResults, how='left', left_on=['Season', 'DayNum', 'Team2'], right_on=['Season','DayNum','TeamID'], suffixes=('', '_2'))\n",
    "        self.col = self.games.drop(['Pred'], axis=1).columns\n",
    "        \n",
    "\n",
    "        self.sub = self.data['SampleSubmissionStage1']\n",
    "        self.sub['Season'] = self.sub['ID'].map(lambda x: x.split('_')[0]).astype(int)\n",
    "        self.sub['Team1'] = self.sub['ID'].map(lambda x: x.split('_')[1]).astype(int)\n",
    "        self.sub['Team2'] = self.sub['ID'].map(lambda x: x.split('_')[2]).astype(int)\n",
    "\n",
    "        last_day = self.teamsResults.groupby(['Season', 'TeamID'])['DayNum'].max().reset_index()\n",
    "        last_day = last_day.rename(columns={'DayNum': 'LastDayNum'})\n",
    "        # Merge last day for Team1\n",
    "        self.sub = pd.merge(self.sub, last_day, how='left', left_on=['Season', 'Team1'], right_on=['Season', 'TeamID'])\n",
    "        self.sub = self.sub.rename(columns={'LastDayNum': 'LastDayNum1'})\n",
    "        self.sub = self.sub.drop('TeamID', axis=1)\n",
    "\n",
    "        # Merge last day for Team2\n",
    "        self.sub = pd.merge(self.sub, last_day, how='left', left_on=['Season', 'Team2'], right_on=['Season', 'TeamID'])\n",
    "        self.sub = self.sub.rename(columns={'LastDayNum': 'LastDayNum2'})\n",
    "        self.sub = self.sub.drop('TeamID', axis=1)\n",
    "\n",
    "        self.sub = pd.merge(self.sub, self.teamsResults, how='left', left_on=['Season', 'LastDayNum1', 'Team1'], right_on=['Season','DayNum','TeamID'], suffixes=('', '_1'))\n",
    "        self.sub = pd.merge(self.sub, self.teamsResults, how='left', left_on=['Season', 'LastDayNum2', 'Team2'], right_on=['Season','DayNum','TeamID'], suffixes=('', '_2'))\n",
    "    \n",
    "   \n",
    "\n",
    "    def getTeamStats(self, df):\n",
    "        def calculate_perc(made, att):\n",
    "            if att == 0:\n",
    "                return 0.0  # Avoid division by zero\n",
    "            return np.round(made / att, 3)\n",
    "        \n",
    "        winF =  ['WTeamID','LTeamID','Season','DayNum','WScore','LScore','WLoc','NumOT', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF','LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n",
    "        loseF = ['LTeamID','WTeamID','Season','DayNum','LScore','WScore','WLoc','NumOT', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF','WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']\n",
    "\n",
    "        winners = df[winF]\n",
    "        losers = df[loseF]\n",
    "        teamF =     ['FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "        opponentF = ['OFGM', 'OFGA', 'OFGM3', 'OFGA3', 'OFTM', 'OFTA', 'OOR', 'ODR', 'OAst', 'OTO', 'OStl', 'OBlk', 'OPF']\n",
    "        winners.columns = ['TeamID','OpponentID','Season','DayNum','Score','OppScore','WLoc','NumOT'] + teamF + opponentF\n",
    "        losers.columns =  ['TeamID','OpponentID','Season','DayNum','Score','OppScore','WLoc','NumOT'] + teamF + opponentF\n",
    "\n",
    "        winners['Pred'] = 1\n",
    "        losers['Pred'] = 0\n",
    "\n",
    "        # 1 = away, 2 = home, 3 = neutral\n",
    "        winners['WLoc'] = winners['WLoc'].map({'A': -1, 'H': 1, 'N': 0}) #WLoc is winner location, looser is opposite\n",
    "        losers['WLoc'] = losers['WLoc'].map({'A': 1, 'H': -1, 'N': 0})\n",
    "        teamF.append('WLoc')\n",
    "\n",
    "        concatDf = pd.concat((winners, losers), axis=0, ignore_index=True)\n",
    "        concatDf['Margin'] = concatDf['Score'] - concatDf['OppScore']\n",
    "        concatDf['NumOT'] = concatDf['NumOT'] > 0\n",
    "        teamF.append('NumOT')\n",
    "\n",
    "        # calculate features\n",
    "        concatDf['FGP2'] = concatDf.apply(lambda row: calculate_perc(row['FGM']-row['FGM3'], row['FGA']-row['FGA3']), axis=1) \n",
    "        concatDf['FGP'] = concatDf.apply(lambda row: calculate_perc(row['FGM'], row['FGA']), axis=1)  #field goals made\n",
    "        concatDf['FGP3'] = concatDf.apply(lambda row: calculate_perc(row['FGM3'], row['FGA3']), axis=1)\n",
    "        concatDf['FTP'] = concatDf.apply(lambda row: calculate_perc(row['FTM'], row['FTA']), axis=1)\n",
    "        teamF += ['FGP2', 'FGP', 'FGP3', 'FTP']\n",
    "\n",
    "        concatDf['OFGP2'] = concatDf.apply(lambda row: calculate_perc(row['OFGM']-row['OFGM3'], row['OFGA']-row['OFGA3']), axis=1) \n",
    "        concatDf['OFGP'] = concatDf.apply(lambda row: calculate_perc(row['OFGM'], row['OFGA']), axis=1)  #field goals made\n",
    "        concatDf['OFGP3'] = concatDf.apply(lambda row: calculate_perc(row['FGM3'], row['OFGA3']), axis=1)\n",
    "        concatDf['OFTP'] = concatDf.apply(lambda row: calculate_perc(row['OFTM'], row['OFTA']), axis=1)\n",
    "        opponentF+=['OFGP2', 'OFGP', 'OFGP3', 'OFTP']\n",
    "\n",
    "        # diff features\n",
    "        concatDf['FGP2_Diff'] = concatDf['FGP2'] - concatDf['OFGP2']\n",
    "        concatDf['FGP_Diff'] = concatDf['FGP'] - concatDf['OFGP']\n",
    "        concatDf['FGP3_Diff'] = concatDf['FGP3'] - concatDf['OFGP3']\n",
    "        concatDf['FTP_Diff'] = concatDf['FTP'] - concatDf['OFTP']\n",
    "\n",
    "        concatDf['OR_Diff'] = concatDf['OR'] - concatDf['OOR']\n",
    "        concatDf['DR_Diff'] = concatDf['DR'] - concatDf['ODR']\n",
    "        concatDf['Ast_Diff'] = concatDf['Ast'] - concatDf['OAst']\n",
    "        concatDf['TO_Diff'] = concatDf['TO'] - concatDf['OTO']\n",
    "        concatDf['Stl_Diff'] = concatDf['Stl'] - concatDf['OStl']\n",
    "        concatDf['Blk_Diff'] = concatDf['Blk'] - concatDf['OBlk']\n",
    "        concatDf['PF_Diff'] = concatDf['PF'] - concatDf['OPF']\n",
    "        diffF = ['Margin','FGP2_Diff', 'FGP_Diff', 'FGP3_Diff', 'FTP_Diff', 'OR_Diff', 'DR_Diff', 'Ast_Diff', 'TO_Diff', 'Stl_Diff', 'Blk_Diff', 'PF_Diff']\n",
    "\n",
    "        # seeds\n",
    "        concatDf['IDTeam'] = concatDf.apply(lambda r: '_'.join(map(str, [r['Season'], r['TeamID']])), axis=1)\n",
    "        concatDf['IDTeamOpp'] = concatDf.apply(lambda r: '_'.join(map(str, [r['Season'], r['OpponentID']])), axis=1)\n",
    "        concatDf['TeamSeed'] = concatDf['IDTeam'].map(self.seeds).fillna(0)\n",
    "        concatDf['Opp2Seed'] = concatDf['IDTeamOpp'].map(self.seeds).fillna(0)\n",
    "        concatDf['SeedDiff'] = concatDf['TeamSeed'] - concatDf['Opp2Seed']\n",
    "        diffF.append('SeedDiff')\n",
    "        teamF.append('TeamSeed')\n",
    "        opponentF.append('Opp2Seed')\n",
    "        concatDf.drop(['IDTeam', 'IDTeamOpp'], axis=1, inplace=True)\n",
    "\n",
    "        gameF = teamF + opponentF + diffF\n",
    "\n",
    "        # create lag features\n",
    "        # log: wloc, \n",
    "        concatDf = concatDf.sort_values(['Season', 'TeamID', 'DayNum'])\n",
    "        concatDf['cum_wins'] = concatDf.groupby(['Season', 'TeamID'])['Pred'].apply(lambda x: x.cumsum().shift(1)).reset_index(level=[0, 1], drop=True).fillna(0)\n",
    "        concatDf['cum_games'] = concatDf.groupby(['Season', 'TeamID'])['DayNum'].cumcount().fillna(1)#.shift(1) + 1\n",
    "        concatDf['win_pct'] = (concatDf['cum_wins'] / concatDf['cum_games']).fillna(0)\n",
    "        \n",
    "        meanF = [ 'cum_wins', 'cum_games', 'win_pct'] # season stats\n",
    "        for f in ['Pred']+teamF+diffF:\n",
    "            print(f)\n",
    "            for i in [3]:#,5,10]:\n",
    "                concatDf, mF = self.getLastNGamesAverage(concatDf, i, f)\n",
    "                meanF.append(mF)\n",
    "            for i in [3]:#,5,10]:\n",
    "                concatDf, mF = self.getLastNGamesStd(concatDf, i, f)\n",
    "                meanF.append(mF)\n",
    "            #for i in [3,5,10]:\n",
    "            #    concatDf, mF = self.getLastNGamesMax(concatDf, i, f)\n",
    "            #    meanF.append(mF)\n",
    "\n",
    "        self.featPerTeam = meanF + ['TeamID', 'Season', 'DayNum','TeamSeed'] \n",
    "\n",
    "        return concatDf[self.featPerTeam]\n",
    "    \n",
    "    def getLastNGamesAverage(self, df, n, f):\n",
    "            df[f+'_last_'+str(n)+'_games_avg'] = df.groupby(['Season', 'TeamID'])[f].apply(lambda x: x.rolling(n, min_periods=1).mean().shift(1)).reset_index(level=[0, 1], drop=True).fillna(0)\n",
    "            return df, f+'_last_'+str(n)+'_games_avg'\n",
    "    def getLastNGamesStd(self, df, n, f):\n",
    "            df[f+'_last_'+str(n)+'_games_std'] = df.groupby(['Season', 'TeamID'])[f].apply(lambda x: x.rolling(n, min_periods=1).std().shift(1)).reset_index(level=[0, 1], drop=True).fillna(0)\n",
    "            return df, f+'_last_'+str(n)+'_games_std'\n",
    "    def getLastNGamesMin(self, df, n, f):\n",
    "            df[f+'_last_'+str(n)+'_games_min'] = df.groupby(['Season', 'TeamID'])[f].apply(lambda x: x.rolling(n, min_periods=1).min().shift(1)).reset_index(level=[0, 1], drop=True).fillna(0)\n",
    "            return df, f+'_last_'+str(n)+'_games_min'\n",
    "    def getLastNGamesMax(self, df, n, f):\n",
    "            df[f+'_last_'+str(n)+'_games_max'] = df.groupby(['Season', 'TeamID'])[f].apply(lambda x: x.rolling(n, min_periods=1).max().shift(1)).reset_index(level=[0, 1], drop=True).fillna(0)\n",
    "            return df, f+'_last_'+str(n)+'_games_max'\n",
    "\n",
    "\n",
    "    def get_xbg(self):\n",
    "      return XGBRegressor(\n",
    "        max_depth=5,  \n",
    "        colsample_bytree=0.5, \n",
    "        subsample=0.8, \n",
    "        n_estimators=3000,  \n",
    "        learning_rate=0.1, \n",
    "        early_stopping_rounds=25,\n",
    "        objective='reg:logistic',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=5\n",
    "        #eval_metric= \"rmse\"\n",
    "      )\n",
    "    def scaled_data(self, input_data):\n",
    "        X_scaled = self.scaler.fit_transform(input_data)\n",
    "        return X_scaled\n",
    "    def impute_data(self, input_data):\n",
    "        X_imputed = self.imputer.fit_transform(input_data)\n",
    "        return X_imputed\n",
    "\n",
    "    def train_model(self, model, df):\n",
    "        X = df[self.col].reset_index(drop=True)#.fillna(-1)\n",
    "        y = df['Pred']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=100)\n",
    "        train_preds = model.predict(X_train).clip(0.001, 0.999)\n",
    "        test_preds0 = model.predict(X_test).clip(0.001, 0.999)\n",
    "\n",
    "        print(f'Log Loss (Train/Test): {log_loss(y_train, train_preds):.4f}, {log_loss(y_test, test_preds0):.4f}')\n",
    "        print(f'Brier Score (Train/Test): {brier_score_loss(y_train, train_preds):.4f}, {brier_score_loss(y_test, test_preds0):.4f}')\n",
    "        print(f'MSE (TrainTest): {mean_squared_error(y_train, train_preds):.4f}, {mean_squared_error(y_test, test_preds0):.4f}')\n",
    "\n",
    "        # Plot ROC Curve for the calibration set.\n",
    "        self.plot_roc_curve(y_test, test_preds0, \"Calibration Set ROC Curve\")\n",
    "\n",
    "        feature_importances = model.feature_importances_\n",
    "        feature_names = self.col\n",
    "        self.plot_feature_importance(feature_importances, feature_names)\n",
    "\n",
    "        self.plot_calibration_curve(y_test, test_preds0)\n",
    "\n",
    "        # Plot the distribution of calibrated predictions.\n",
    "        self.plot_prediction_distribution(y_test, \"Distribution of Test Predictions\")\n",
    "\n",
    "    def train_model_cv(self, model, df):\n",
    "        X = df[self.col].reset_index(drop=True)\n",
    "        y = df['Pred'].reset_index(drop=True)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_mse_scores = []\n",
    "        cv_logloss_scores = []\n",
    "        cv_test_mse_scores = []\n",
    "        cv_test_logloss_scores = []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "            # start with fresh model every time!\n",
    "            clonsed_model = clone(model)\n",
    "            clonsed_model.fit(X_train_cv, y_train_cv, eval_set=[(X_test_cv, y_test_cv)], verbose=100)\n",
    "            train_preds_cv = clonsed_model.predict(X_train_cv).clip(0.001, 0.999)\n",
    "            test_preds_cv = clonsed_model.predict(X_test_cv).clip(0.001, 0.999)\n",
    "\n",
    "\n",
    "            train_mse_cv = mean_squared_error(y_train_cv, train_preds_cv)\n",
    "            train_logloss_cv = log_loss(y_train_cv, train_preds_cv)\n",
    "            test_mse_cv = mean_squared_error(y_test_cv, test_preds_cv)\n",
    "            test_logloss_cv = log_loss(y_test_cv, test_preds_cv)\n",
    "\n",
    "            cv_mse_scores.append(train_mse_cv)\n",
    "            cv_logloss_scores.append(train_logloss_cv)\n",
    "            cv_test_mse_scores.append(test_mse_cv)\n",
    "            cv_test_logloss_scores.append(test_logloss_cv)\n",
    "\n",
    "        \n",
    "        print(f'Cross-validated MSE: {np.mean(cv_mse_scores):.4f},{np.mean(cv_test_mse_scores):.4f}')\n",
    "        print(f'Cross-validated LogLoss: {np.mean(cv_logloss_scores):.4f},{np.mean(cv_test_logloss_scores):.4f}')\n",
    "        print(\"Test cv array: \",cv_test_mse_scores)\n",
    "\n",
    "    def predict_submission(self, output_file='submission.csv'):\n",
    "        sub_X = self.sub[self.col].fillna(-1)\n",
    "        sub_X_imputed = self.imputer.transform(sub_X)\n",
    "        sub_X_scaled = self.scaler.transform(sub_X_imputed)\n",
    "\n",
    "        preds = self.model.predict(sub_X_scaled).clip(0.001, 0.999)\n",
    "        preds_calibrated = self.calibration_model.predict(preds.reshape(-1, 1)).clip(0.001, 0.999)\n",
    "\n",
    "        self.sub['Pred'] = preds_calibrated\n",
    "        self.sub[['ID', 'Pred']].to_csv(output_file, index=False)\n",
    "        print(f\"Submission file saved to {output_file}\")\n",
    "\n",
    "    def plot_feature_importance(self, importances, feature_names, top_n=20):\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).head(top_n)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df, palette='viridis')\n",
    "        plt.title('Top {} Feature Importances'.format(top_n))\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_calibration_curve(self, y_true, y_proba, n_bins=10):\n",
    "\n",
    "        combined = np.stack([y_proba, y_true], axis=-1)\n",
    "        combined = combined[np.argsort(combined[:, 0])]\n",
    "        sorted_probas = combined[:, 0]\n",
    "        sorted_true = combined[:, 1]\n",
    "\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_midpoints = bins[:-1] + (bins[1] - bins[0]) / 2\n",
    "        bin_assignments = np.digitize(sorted_probas, bins) - 1\n",
    "\n",
    "        bin_sums = np.bincount(bin_assignments, weights=sorted_probas, minlength=n_bins)\n",
    "        bin_true = np.bincount(bin_assignments, weights=sorted_true, minlength=n_bins)\n",
    "        bin_total = np.bincount(bin_assignments, minlength=n_bins)\n",
    "\n",
    "        fraction_of_positives = bin_true / bin_total\n",
    "        fraction_of_positives[np.isnan(fraction_of_positives)] = 0\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(bin_midpoints, fraction_of_positives, marker='o', label='Calibration Curve')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.ylabel('Fraction of Positives')\n",
    "        plt.title('Calibration Curve')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_prediction_distribution(self, predictions, title=\"Distribution of Predictions\"):\n",
    "        \"\"\"Plots the distribution of model predictions.\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(predictions, kde=True, color='skyblue')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curve(self, y_true, y_proba, title=\"ROC Curve\"):\n",
    "      \"\"\"Plots the Receiver Operating Characteristic (ROC) curve.\"\"\"\n",
    "      fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "\n",
    "      plt.figure(figsize=(8, 6))\n",
    "      plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "      plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "      plt.xlabel('False Positive Rate')\n",
    "      plt.ylabel('True Positive Rate')\n",
    "      plt.title(title)\n",
    "      plt.xlim([0.0, 1.0])\n",
    "      plt.ylim([0.0, 1.05])\n",
    "      plt.legend(loc=\"lower right\")\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "    def train_model_base_xgboost(self):\n",
    "        model = self.get_xbg()\n",
    "        self.train_model_cv(model, self.games.fillna(-1))\n",
    "        self.train_model(model, self.games.fillna(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 79.711206,
     "end_time": "2025-02-20T05:14:16.779969",
     "exception": false,
     "start_time": "2025-02-20T05:12:57.068763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = TournamentPredictor(data_dir)\n",
    "predictor.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "lags only 3\n",
    "Cross-validated MSE: 0.1708,0.1827\n",
    "Cross-validated LogLoss: 0.5104,0.5394\n",
    "Test cv array:  [0.18181727767832848, 0.18038723020479824, 0.1835351904420142, 0.18512420426450932, 0.18283504670415224]\n",
    "\n",
    "test difference rather than feat_1 feat_2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(\n",
    "        max_depth=5,  \n",
    "        colsample_bytree=0.5, \n",
    "        subsample=0.8, \n",
    "        n_estimators=3000,  \n",
    "        learning_rate=0.1, \n",
    "        early_stopping_rounds=50,\n",
    "        objective='reg:logistic',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=30\n",
    "        #eval_metric= \"rmse\"\n",
    "      )\n",
    "data = predictor.games.fillna(-1)\n",
    "predictor.train_model_cv(xgb, data)\n",
    "predictor.train_model(xgb, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictor.teamsResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DayNum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "df.loc[(df['TeamID'] == 1103) & (df['Season'] == 2024)].sort_values(by='DayNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(df.loc[(df['TeamID'] == 1104) & (df['Season'] == 2024)].sort_values(by='DayNum')['Margin'])\n",
    "plot_pacf(df.loc[(df['TeamID'] == 1104) & (df['Season'] == 2024)].sort_values(by='DayNum')['Margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11018643,
     "sourceId": 91497,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 86.992117,
   "end_time": "2025-02-20T05:14:17.825622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-20T05:12:50.833505",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
